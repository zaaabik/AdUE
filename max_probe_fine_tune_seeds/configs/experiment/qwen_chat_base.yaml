# @package _global_

# to execute this experiment run:
# python train.py experiment=example
model_name: Qwen2.5-7B-Instruct
base_model: true
add_space: true
model:
  use_flash_attention_2:
    _target_: transformers.utils.is_flash_attn_2_available
  torch_dtype:
    _target_: hydra.utils.get_object
    path: torch.bfloat16
adapter:
  path: Qwen/Qwen2.5-7B-Instruct
seed: 0
classifier_name: lm_head
pooling:
  _target_: src.utils.pooling.LastTokenPooling
  layer_number: -1
map_targets: true