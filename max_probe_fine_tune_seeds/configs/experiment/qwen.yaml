# @package _global_

# to execute this experiment run:
# python train.py experiment=example
model_name: qwen
model:
  use_flash_attention_2:
    _target_: transformers.utils.is_flash_attn_2_available
  torch_dtype:
    _target_: hydra.utils.get_object
    path: torch.bfloat16
adapter:
  path: ${oc.env:BASE_WEIGHT_PATH}/${model_name}_runs_seeds/${data.name}/${seed}/
seed: 0
classifier_name: score
pooling:
  _target_: src.utils.pooling.LastTokenPooling
  layer_number: -1