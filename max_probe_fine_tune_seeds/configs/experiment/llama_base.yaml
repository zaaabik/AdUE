# @package _global_

# to execute this experiment run:
# python train.py experiment=example
model_name: llama
model:
  use_flash_attention_2:
    _target_: transformers.utils.is_flash_attn_2_available
  torch_dtype:
    _target_: hydra.utils.get_object
    path: torch.bfloat16
adapter:
  path: meta-llama/Llama-2-7b-hf
seed: 0
classifier_name: score
pooling:
  _target_: src.utils.pooling.LastTokenPooling
  layer_number: -1