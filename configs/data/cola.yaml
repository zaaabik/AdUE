_target_: src.data.text_module.TextDataModule
_recursive_: false
num_labels: 2
dataset:
  _target_: src.dataset.sequence_classification.CoLa
batch_size: 32 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
num_workers: 0
pin_memory: False
collate_fn:
  _target_: transformers.DataCollatorWithPadding
  return_tensors: pt
  padding: longest
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${base_architecture}
